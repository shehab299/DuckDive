<!--?xml version="1.0" encoding="UTF-8"?-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
 <head>
  <title>Statistics  authors/titles "new"</title>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215">
  <link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
  <link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css">
  <link rel="alternate" type="application/rss+xml" title="Statistics " href="http://arxiv.org/rss/stat">
  <script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>
 </head>
 <body class="with-cu-identity">
  <div id="cu-identity">
   <div id="cu-logo">
    <a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0"></a>
   </div>
   <div id="support-ack">
    <a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br>
      the Simons Foundation and member institutions.</a>
   </div>
  </div>
  <div id="header">
   <h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/stat/recent">stat</a></h1>
   <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
     <div class="field has-addons">
      <div class="control">
       <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
       <p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
      </div>
      <div class="control">
       <div class="select is-small">
        <select name="searchtype" aria-label="Field to search"> <option value="all" selected>All fields</option> <option value="title">Title</option> <option value="author">Author</option> <option value="abstract">Abstract</option> <option value="comments">Comments</option> <option value="journal_ref">Journal reference</option> <option value="acm_class">ACM classification</option> <option value="msc_class">MSC classification</option> <option value="report_num">Report number</option> <option value="paper_id">arXiv identifier</option> <option value="doi">DOI</option> <option value="orcid">ORCID</option> <option value="author_id">arXiv author ID</option> <option value="help">Help pages</option> <option value="full_text">Full text</option> </select>
       </div>
      </div><button class="button is-small is-cul-darker">Search</button>
     </div>
    </form>
   </div>
  </div>
  <div id="content">
   <div id="dlpage">
    <h1>Statistics</h1>
    <h2>New submissions</h2>
    <div class="list-dateline">
     Submissions received from Mon 29 Apr 24 to Tue 30 Apr 24, announced Wed, 1 May 24
    </div>
    <ul>
     <li><a href="/list/stat/new?skip=0&amp;show=2000">New submissions</a></li>
     <li><a href="#item24">Cross-lists</a></li>
     <li><a href="#item42">Replacements</a></li>
    </ul><small>[ total of 70 entries: <b>1-70</b> ]</small>
    <br><small>[ showing up to 2000 entries per page: <a href="/list/stat/new?skip=0&amp;show=1000">fewer</a> | <font color="#999999">more</font> ]</small>
    <br>
    <h3>New submissions for Wed, 1 May 24</h3>
    <dl>
     <dt>
      <a name="item1">[1]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19053" title="Abstract">arXiv:2404.19053</a> [<a href="/pdf/2404.19053" title="Download PDF">pdf</a>, <a href="/format/2404.19053" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Fast Adaptive Fourier Integration for Spectral Densities of Gaussian Processes
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Beckman%2C+P+G">Paul G. Beckman</a>, <a href="/search/stat?searchtype=author&amp;query=Geoga%2C+C+J">Christopher J. Geoga</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Numerical Analysis (math.NA)
       </div>
       <p class="mathjax">The specification of a covariance function is of paramount importance when employing Gaussian process models, but the requirement of positive definiteness severely limits those used in practice. Designing flexible stationary covariance functions is, however, straightforward in the spectral domain, where one needs only to supply a positive and symmetric spectral density. In this work, we introduce an adaptive integration framework for efficiently and accurately evaluating covariance functions and their derivatives at irregular locations directly from \textit{any} continuous, integrable spectral density. In order to make this approach computationally tractable, we employ high-order panel quadrature, the nonuniform fast Fourier transform, and a Nyquist-informed panel selection heuristic, and derive novel algebraic truncation error bounds which are used to monitor convergence. As a result, we demonstrate several orders of magnitude speedup compared to naive uniform quadrature approaches, allowing us to evaluate covariance functions from slowly decaying, singular spectral densities at millions of locations to a user-specified tolerance in seconds on a laptop. We then apply our methodology to perform gradient-based maximum likelihood estimation using a previously numerically infeasible long-memory spectral model for wind velocities below the atmospheric boundary layer.</p>
      </div>
     </dd>
     <dt>
      <a name="item2">[2]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19073" title="Abstract">arXiv:2404.19073</a> [<a href="/pdf/2404.19073" title="Download PDF">pdf</a>, <a href="/format/2404.19073" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Learning Sparse High-Dimensional Matrix-Valued Graphical Models From Dependent Data
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Tugnait%2C+J+K">Jitendra K Tugnait</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 16 pages, 2 figures, 1 table
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)
       </div>
       <p class="mathjax">We consider the problem of inferring the conditional independence graph (CIG) of a sparse, high-dimensional, stationary matrix-variate Gaussian time series. All past work on high-dimensional matrix graphical models assumes that independent and identically distributed (i.i.d.) observations of the matrix-variate are available. Here we allow dependent observations. We consider a sparse-group lasso-based frequency-domain formulation of the problem with a Kronecker-decomposable power spectral density (PSD), and solve it via an alternating direction method of multipliers (ADMM) approach. The problem is bi-convex which is solved via flip-flop optimization. We provide sufficient conditions for local convergence in the Frobenius norm of the inverse PSD estimators to the true value. This result also yields a rate of convergence. We illustrate our approach using numerical examples utilizing both synthetic and real data.</p>
      </div>
     </dd>
     <dt>
      <a name="item3">[3]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19118" title="Abstract">arXiv:2404.19118</a> [<a href="/pdf/2404.19118" title="Download PDF">pdf</a>, <a href="/format/2404.19118" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Identification and estimation of causal effects using non-concurrent controls in platform trials
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Santacatterina%2C+M">Michele Santacatterina</a>, <a href="/search/stat?searchtype=author&amp;query=Giron%2C+F+M">Federico Macchiavelli Giron</a>, <a href="/search/stat?searchtype=author&amp;query=Zhang%2C+X">Xinyi Zhang</a>, <a href="/search/stat?searchtype=author&amp;query=Diaz%2C+I">Ivan Diaz</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>
       </div>
       <p class="mathjax">Platform trials are multi-arm designs that simultaneously evaluate multiple treatments for a single disease within the same overall trial structure. Unlike traditional randomized controlled trials, they allow treatment arms to enter and exit the trial at distinct times while maintaining a control arm throughout. This control arm comprises both concurrent controls, where participants are randomized concurrently to either the treatment or control arm, and non-concurrent controls, who enter the trial when the treatment arm under study is unavailable. While flexible, platform trials introduce a unique challenge with the use of non-concurrent controls, raising questions about how to efficiently utilize their data to estimate treatment effects. Specifically, what estimands should be used to evaluate the causal effect of a treatment versus control? Under what assumptions can these estimands be identified and estimated? Do we achieve any efficiency gains? In this paper, we use structural causal models and counterfactuals to clarify estimands and formalize their identification in the presence of non-concurrent controls in platform trials. We also provide outcome regression, inverse probability weighting, and doubly robust estimators for their estimation. We discuss efficiency gains, demonstrate their performance in a simulation study, and apply them to the ACTT platform trial, resulting in a 20% improvement in precision.</p>
      </div>
     </dd>
     <dt>
      <a name="item4">[4]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19127" title="Abstract">arXiv:2404.19127</a> [<a href="/pdf/2404.19127" title="Download PDF">pdf</a>, <a href="/format/2404.19127" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> A model-free subdata selection method for classification
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Singh%2C+R">Rakhi Singh</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (stat.ML)
       </div>
       <p class="mathjax">Subdata selection is a study of methods that select a small representative sample of the big data, the analysis of which is fast and statistically efficient. The existing subdata selection methods assume that the big data can be reasonably modeled using an underlying model, such as a (multinomial) logistic regression for classification problems. These methods work extremely well when the underlying modeling assumption is correct but often yield poor results otherwise. In this paper, we propose a model-free subdata selection method for classification problems, and the resulting subdata is called PED subdata. The PED subdata uses decision trees to find a partition of the data, followed by selecting an appropriate sample from each component of the partition. Random forests are used for analyzing the selected subdata. Our method can be employed for a general number of classes in the response and for both categorical and continuous predictors. We show analytically that the PED subdata results in a smaller Gini than a uniform subdata. Further, we demonstrate that the PED subdata has higher classification accuracy than other competing methods through extensive simulated and real datasets.</p>
      </div>
     </dd>
     <dt>
      <a name="item5">[5]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19145" title="Abstract">arXiv:2404.19145</a> [<a href="/pdf/2404.19145" title="Download PDF">pdf</a>, <a href="/format/2404.19145" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Orthogonal Bootstrap: Efficient Simulation of Input Uncertainty
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Liu%2C+K">Kaizhao Liu</a>, <a href="/search/stat?searchtype=author&amp;query=Blanchet%2C+J">Jose Blanchet</a>, <a href="/search/stat?searchtype=author&amp;query=Ying%2C+L">Lexing Ying</a>, <a href="/search/stat?searchtype=author&amp;query=Lu%2C+Y">Yiping Lu</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Statistics Theory (math.ST); Machine Learning (stat.ML)
       </div>
       <p class="mathjax">Bootstrap is a popular methodology for simulating input uncertainty. However, it can be computationally expensive when the number of samples is large. We propose a new approach called \textbf{Orthogonal Bootstrap} that reduces the number of required Monte Carlo replications. We decomposes the target being simulated into two parts: the \textit{non-orthogonal part} which has a closed-form result known as Infinitesimal Jackknife and the \textit{orthogonal part} which is easier to be simulated. We theoretically and numerically show that Orthogonal Bootstrap significantly reduces the computational cost of Bootstrap while improving empirical accuracy and maintaining the same width of the constructed interval.</p>
      </div>
     </dd>
     <dt>
      <a name="item6">[6]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19157" title="Abstract">arXiv:2404.19157</a> [<a href="/pdf/2404.19157" title="Download PDF">pdf</a>, <a href="/format/2404.19157" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Scalable Bayesian Inference in the Era of Deep Learning: From Gaussian Processes to Deep Neural Networks
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Antoran%2C+J">Javier Antoran</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> PhD Thesis, University of Cambridge
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)
       </div>
       <p class="mathjax">Large neural networks trained on large datasets have become the dominant paradigm in machine learning. These systems rely on maximum likelihood point estimates of their parameters, precluding them from expressing model uncertainty. This may result in overconfident predictions and it prevents the use of deep learning models for sequential decision making. This thesis develops scalable methods to equip neural networks with model uncertainty. In particular, we leverage the linearised Laplace approximation to equip pre-trained neural networks with the uncertainty estimates provided by their tangent linear models. This turns the problem of Bayesian inference in neural networks into one of Bayesian inference in conjugate Gaussian-linear models. Alas, the cost of this remains cubic in either the number of network parameters or in the number of observations times output dimensions. By assumption, neither are tractable. We address this intractability by using stochastic gradient descent (SGD) -- the workhorse algorithm of deep learning -- to perform posterior sampling in linear models and their convex duals: Gaussian processes. With this, we turn back to linearised neural networks, finding the linearised Laplace approximation to present a number of incompatibilities with modern deep learning practices -- namely, stochastic optimisation, early stopping and normalisation layers -- when used for hyperparameter learning. We resolve these and construct a sample-based EM algorithm for scalable hyperparameter learning with linearised neural networks. We apply the above methods to perform linearised neural network inference with ResNet-50 (25M parameters) trained on Imagenet (1.2M observations and 1000 output dimensions). Additionally, we apply our methods to estimate uncertainty for 3d tomographic reconstructions obtained with the deep image prior network.</p>
      </div>
     </dd>
     <dt>
      <a name="item7">[7]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19176" title="Abstract">arXiv:2404.19176</a> [<a href="/pdf/2404.19176" title="Download PDF">pdf</a>, <a href="/format/2404.19176" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Detecting Spectral Breaks in Spiked Covariance Models
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/math?searchtype=author&amp;query=D%C3%B6rnemann%2C+N">Nina Dörnemann</a>, <a href="/search/math?searchtype=author&amp;query=Paul%2C+D">Debashis Paul</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 42 pages
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>
       </div>
       <p class="mathjax">In this paper, the key objects of interest are the sequential covariance matrices $\mathbf{S}_{n,t}$ and their largest eigenvalues. Here, the matrix $\mathbf{S}_{n,t}$ is computed as the empirical covariance associated with observations $\{\mathbf{x}_1,\ldots,\mathbf{x}_{ \lfloor nt \rfloor } \}$, for $t\in [0,1]$. The observations $\mathbf{x}_1,\ldots,\mathbf{x}_n$ are assumed to be i.i.d. $p$-dimensional vectors with zero mean, and a covariance matrix that is a fixed-rank perturbation of the identity matrix. Treating $\{ \mathbf{S}_{n,t}\}_{t \in [0,1]}$ as a matrix-valued stochastic process indexed by $t$, we study the behavior of the largest eigenvalues of $\mathbf{S}_{n,t}$, as $t$ varies, with $n$ and $p$ increasing simultaneously, so that $p/n \to y \in (0,1)$. As a key contribution of this work, we establish the weak convergence of the stochastic process corresponding to the sample spiked eigenvalues, if their population counterparts exceed the critical phase-transition threshold. Our analysis of the limiting process is fully comprehensive revealing, in general, non-Gaussian limiting processes. <br>
        As an application, we consider a class of change-point problems, where the interest is in detecting structural breaks in the covariance caused by a change in magnitude of the spiked eigenvalues. For this purpose, we propose two different maximal statistics corresponding to centered spiked eigenvalues of the sequential covariances. We show the existence of limiting null distributions for these statistics, and prove consistency of the test under fixed alternatives. Moreover, we compare the behavior of the proposed tests through a simulation study.</p>
      </div>
     </dd>
     <dt>
      <a name="item8">[8]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19196" title="Abstract">arXiv:2404.19196</a> [<a href="/pdf/2404.19196" title="Download PDF">pdf</a>, <a href="/format/2404.19196" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Tail Asymptotic of Heavy-Tail Risks with Elliptical Copula
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/math?searchtype=author&amp;query=Wang%2C+K">Kai Wang</a>, <a href="/search/math?searchtype=author&amp;query=Ling%2C+C">Chengxiu Ling</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Probability (math.PR)
       </div>
       <p class="mathjax">We consider a family of multivariate distributions with heavy-tailed margins and the type I elliptical dependence structure. This class of risks is common in finance, insurance, environmental and biostatistic applications. We obtain the asymptotic tail risk probabilities and characterize the multivariate regular variation property. The results demonstrate how the rate of decay of probabilities on tail sets varies in tail sets and the covariance matrix of the elliptical copula. The theoretical results are well illustrated by typical examples and numerical simulations. A real data application shows its advantages in a more flexible dependence structure to characterize joint insurance losses.</p>
      </div>
     </dd>
     <dt>
      <a name="item9">[9]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19220" title="Abstract">arXiv:2404.19220</a> [<a href="/pdf/2404.19220" title="Download PDF">pdf</a>, <a href="/format/2404.19220" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Regression for matrix-valued data via Kronecker products factorization
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Chen%2C+Y">Yin-Jen Chen</a>, <a href="/search/stat?searchtype=author&amp;query=Tang%2C+M">Minh Tang</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)
       </div>
       <p class="mathjax">We study the matrix-variate regression problem $Y_i = \sum_{k} \beta_{1k} X_i \beta_{2k}^{\top} + E_i$ for $i=1,2\dots,n$ in the high dimensional regime wherein the response $Y_i$ are matrices whose dimensions $p_{1}\times p_{2}$ outgrow both the sample size $n$ and the dimensions $q_{1}\times q_{2}$ of the predictor variables $X_i$ i.e., $q_{1},q_{2} \ll n \ll p_{1},p_{2}$. We propose an estimation algorithm, termed KRO-PRO-FAC, for estimating the parameters $\{\beta_{1k}\} \subset \Re^{p_1 \times q_1}$ and $\{\beta_{2k}\} \subset \Re^{p_2 \times q_2}$ that utilizes the Kronecker product factorization and rearrangement operations from Van Loan and Pitsianis (1993). The KRO-PRO-FAC algorithm is computationally efficient as it does not require estimating the covariance between the entries of the $\{Y_i\}$. We establish perturbation bounds between $\hat{\beta}_{1k} -\beta_{1k}$ and $\hat{\beta}_{2k} - \beta_{2k}$ in spectral norm for the setting where either the rows of $E_i$ or the columns of $E_i$ are independent sub-Gaussian random vectors. Numerical studies on simulated and real data indicate that our procedure is competitive, in terms of both estimation error and predictive accuracy, compared to other existing methods.</p>
      </div>
     </dd>
     <dt>
      <a name="item10">[10]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19224" title="Abstract">arXiv:2404.19224</a> [<a href="/pdf/2404.19224" title="Download PDF">pdf</a>, <a href="/format/2404.19224" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Variational approximations of possibilistic inferential models
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Cella%2C+L">Leonardo Cella</a>, <a href="/search/stat?searchtype=author&amp;query=Martin%2C+R">Ryan Martin</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> Comments welcome at <a href="https://researchers.one/articles/24.04.00005">this https URL</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Methodology (stat.ME)
       </div>
       <p class="mathjax">Inferential models (IMs) offer reliable, data-driven, possibilistic statistical inference. But despite IMs' theoretical/foundational advantages, efficient computation in applications is a major challenge. This paper presents a simple and apparently powerful Monte Carlo-driven strategy for approximating the IM's possibility contour, or at least its $\alpha$-level set for a specified $\alpha$. Our proposal utilizes a parametric family that, in a certain sense, approximately covers the credal set associated with the IM's possibility measure, which is reminiscent of variational approximations now widely used in Bayesian statistics.</p>
      </div>
     </dd>
     <dt>
      <a name="item11">[11]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19301" title="Abstract">arXiv:2404.19301</a> [<a href="/pdf/2404.19301" title="Download PDF">pdf</a>, <a href="/ps/2404.19301" title="Download PostScript">ps</a>, <a href="/format/2404.19301" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Statistics and explainability: a fruitful alliance
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Ghidini%2C+V">Valentina Ghidini</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)
       </div>
       <p class="mathjax">In this paper, we propose standard statistical tools as a solution to commonly highlighted problems in the explainability literature. Indeed, leveraging statistical estimators allows for a proper definition of explanations, enabling theoretical guarantees and the formulation of evaluation metrics to quantitatively assess the quality of explanations. This approach circumvents, among other things, the subjective human assessment currently prevalent in the literature. Moreover, we argue that uncertainty quantification is essential for providing robust and trustworthy explanations, and it can be achieved in this framework through classical statistical procedures such as the bootstrap. However, it is crucial to note that while Statistics offers valuable contributions, it is not a panacea for resolving all the challenges. Future research avenues could focus on open problems, such as defining a purpose for the explanations or establishing a statistical framework for counterfactual or adversarial scenarios.</p>
      </div>
     </dd>
     <dt>
      <a name="item12">[12]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19325" title="Abstract">arXiv:2404.19325</a> [<a href="/pdf/2404.19325" title="Download PDF">pdf</a>, <a href="/ps/2404.19325" title="Download PostScript">ps</a>, <a href="/format/2404.19325" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Correcting for confounding in longitudinal experiments: positioning non-linear mixed effects modeling as implementation of standardization using latent conditional exchangeability
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Bartels%2C+C">Christian Bartels</a>, <a href="/search/stat?searchtype=author&amp;query=Scauda%2C+M">Martina Scauda</a>, <a href="/search/stat?searchtype=author&amp;query=Coello%2C+N">Neva Coello</a>, <a href="/search/stat?searchtype=author&amp;query=Dumortier%2C+T">Thomas Dumortier</a>, <a href="/search/stat?searchtype=author&amp;query=Bornkamp%2C+B">Bjoern Bornkamp</a>, <a href="/search/stat?searchtype=author&amp;query=Moffa%2C+G">Giusi Moffa</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Applications (stat.AP)
       </div>
       <p class="mathjax">Non-linear mixed effects modeling and simulation (NLME M&amp;S) is evaluated to be used for standardization with longitudinal data in presence of confounders. Standardization is a well-known method in causal inference to correct for confounding by analyzing and combining results from subgroups of patients. We show that non-linear mixed effects modeling is a particular implementation of standardization that conditions on individual parameters described by the random effects of the mixed effects model. Our motivation is that in pharmacometrics NLME M&amp;S is routinely used to analyze clinical trials and to predict and compare potential outcomes of the same patient population under different treatment regimens. Such a comparison is a causal question sometimes referred to as causal prediction. Nonetheless, NLME M&amp;S is rarely positioned as a method for causal prediction. <br>
        As an example, a simulated clinical trial is used that assumes treatment confounder feedback in which early outcomes can cause deviations from the planned treatment schedule. Being interested in the outcome for the hypothetical situation that patients adhere to the planned treatment schedule, we put assumptions in a causal diagram. From the causal diagram, conditional independence assumptions are derived either using latent conditional exchangeability, conditioning on the individual parameters, or using sequential conditional exchangeability, conditioning on earlier outcomes. Both conditional independencies can be used to estimate the estimand of interest, e.g., with standardization, and they give unbiased estimates.</p>
      </div>
     </dd>
     <dt>
      <a name="item13">[13]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19344" title="Abstract">arXiv:2404.19344</a> [<a href="/pdf/2404.19344" title="Download PDF">pdf</a>, <a href="/ps/2404.19344" title="Download PostScript">ps</a>, <a href="/format/2404.19344" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Data-adaptive structural change-point detection via isolation
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Anastasiou%2C+A">Andreas Anastasiou</a>, <a href="/search/stat?searchtype=author&amp;query=Loizidou%2C+S">Sophia Loizidou</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 33 pages, 4 figures
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>
       </div>
       <p class="mathjax">In this paper, a new data-adaptive method, called DAIS (Data Adaptive ISolation), is introduced for the estimation of the number and the location of change-points in a given data sequence. The proposed method can detect changes in various different signal structures; we focus on the examples of piecewise-constant and continuous, piecewise-linear signals. We highlight, however, that our algorithm can be extended to other frameworks, such as piecewise-quadratic signals. The data-adaptivity of our methodology lies in the fact that, at each step, and for the data under consideration, we search for the most prominent change-point in a targeted neighborhood of the data sequence that contains this change-point with high probability. Using a suitably chosen contrast function, the change-point will then get detected after being isolated in an interval. The isolation feature enhances estimation accuracy, while the data-adaptive nature of DAIS is advantageous regarding, mainly, computational complexity and accuracy. The simulation results presented indicate that DAIS is at least as accurate as state-of-the-art competitors.</p>
      </div>
     </dd>
     <dt>
      <a name="item14">[14]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19367" title="Abstract">arXiv:2404.19367</a> [<a href="/pdf/2404.19367" title="Download PDF">pdf</a>, <a href="/format/2404.19367" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Parametric estimation and LAN property of the birth-death-move process with mutations
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/math?searchtype=author&amp;query=Balsollier%2C+L">Lisa Balsollier</a> (LMJL), <a href="/search/math?searchtype=author&amp;query=Lavancier%2C+F">Frédéric Lavancier</a> (CREST)
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>
       </div>
       <p class="mathjax">A birth-death-move process with mutations is a Markov model for a system of marked particles in interaction, that move over time, with births and deaths. In addition the mark of each particle may also change, which constitutes a mutation. Assuming a parametric form for this model, we derive its likelihood expression and prove its local asymptotic normality. The efficiency and asymptotic distribution of the maximum likelihood estimator, with an explicit expression of its covariance matrix, is deduced. The underlying technical assumptions are showed to be satisfied by several natural parametric specifications. As an application, we leverage this model to analyse the joint dynamics of two types of proteins in a living cell, that are involved in the exocytosis process. Our approach enables to quantify the so-called colocalization phenomenon, answering an important question in microbiology.</p>
      </div>
     </dd>
     <dt>
      <a name="item15">[15]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19465" title="Abstract">arXiv:2404.19465</a> [<a href="/pdf/2404.19465" title="Download PDF">pdf</a>, <a href="/format/2404.19465" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Optimal E-Values for Exponential Families: the Simple Case
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Gr%C3%BCnwald%2C+P">Peter Grünwald</a>, <a href="/search/stat?searchtype=author&amp;query=Lardy%2C+T">Tyron Lardy</a>, <a href="/search/stat?searchtype=author&amp;query=Hao%2C+Y">Yunda Hao</a>, <a href="/search/stat?searchtype=author&amp;query=Bar-Lev%2C+S+K">Shaul K. Bar-Lev</a>, <a href="/search/stat?searchtype=author&amp;query=de+Jong%2C+M">Martijn de Jong</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Statistics Theory (math.ST)
       </div>
       <p class="mathjax">We provide a general condition under which e-variables in the form of a simple-vs.-simple likelihood ratio exist when the null hypothesis is a composite, multivariate exponential family. Such `simple' e-variables are easy to compute and expected-log-optimal with respect to any stopping time. Simple e-variables were previously only known to exist in quite specific settings, but we offer a unifying theorem on their existence for testing exponential families. We start with a simple alternative $Q$ and a regular exponential family null. Together these induce a second exponential family ${\cal Q}$ containing $Q$, with the same sufficient statistic as the null. Our theorem shows that simple e-variables exist whenever the covariance matrices of ${\cal Q}$ and the null are in a certain relation. Examples in which this relation holds include some $k$-sample tests, Gaussian location- and scale tests, and tests for more general classes of natural exponential families.</p>
      </div>
     </dd>
     <dt>
      <a name="item16">[16]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19472" title="Abstract">arXiv:2404.19472</a> [<a href="/pdf/2404.19472" title="Download PDF">pdf</a>, <a href="/format/2404.19472" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Multi-label Classification under Uncertainty: A Tree-based Conformal Prediction Approach
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Tyagi%2C+C">Chhavi Tyagi</a>, <a href="/search/stat?searchtype=author&amp;query=Guo%2C+W">Wenge Guo</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 21 pages, 7 figures; 3 supplementary pages
       </div>
       <div class="list-journal-ref">
        <span class="descriptor">Journal-ref:</span> In COPA 2023 : 12th Symposium on Conformal and Probabilistic Prediction with Applications
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>
       </div>
       <p class="mathjax">Multi-label classification is a common challenge in various machine learning applications, where a single data instance can be associated with multiple classes simultaneously. The current paper proposes a novel tree-based method for multi-label classification using conformal prediction and multiple hypothesis testing. The proposed method employs hierarchical clustering with labelsets to develop a hierarchical tree, which is then formulated as a multiple-testing problem with a hierarchical structure. The split-conformal prediction method is used to obtain marginal conformal $p$-values for each tested hypothesis, and two \textit{hierarchical testing procedures} are developed based on marginal conformal $p$-values, including a hierarchical Bonferroni procedure and its modification for controlling the family-wise error rate. The prediction sets are thus formed based on the testing outcomes of these two procedures. We establish a theoretical guarantee of valid coverage for the prediction sets through proven family-wise error rate control of those two procedures. We demonstrate the effectiveness of our method in a simulation study and two real data analysis compared to other conformal methods for multi-label classification.</p>
      </div>
     </dd>
     <dt>
      <a name="item17">[17]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19494" title="Abstract">arXiv:2404.19494</a> [<a href="/pdf/2404.19494" title="Download PDF">pdf</a>, <a href="/format/2404.19494" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> The harms of class imbalance corrections for machine learning based prediction models: a simulation study
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Carriero%2C+A">Alex Carriero</a>, <a href="/search/stat?searchtype=author&amp;query=Luijken%2C+K">Kim Luijken</a>, <a href="/search/stat?searchtype=author&amp;query=de+Hond%2C+A">Anne de Hond</a>, <a href="/search/stat?searchtype=author&amp;query=Moons%2C+K+G">Karel GM Moons</a>, <a href="/search/stat?searchtype=author&amp;query=van+Calster%2C+B">Ben van Calster</a>, <a href="/search/stat?searchtype=author&amp;query=van+Smeden%2C+M">Maarten van Smeden</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>
       </div>
       <p class="mathjax">Risk prediction models are increasingly used in healthcare to aid in clinical decision making. In most clinical contexts, model calibration (i.e., assessing the reliability of risk estimates) is critical. Data available for model development are often not perfectly balanced with respect to the modeled outcome (i.e., individuals with vs. without the event of interest are not equally represented in the data). It is common for researchers to correct this class imbalance, yet, the effect of such imbalance corrections on the calibration of machine learning models is largely unknown. We studied the effect of imbalance corrections on model calibration for a variety of machine learning algorithms. Using extensive Monte Carlo simulations we compared the out-of-sample predictive performance of models developed with an imbalance correction to those developed without a correction for class imbalance across different data-generating scenarios (varying sample size, the number of predictors and event fraction). Our findings were illustrated in a case study using MIMIC-III data. In all simulation scenarios, prediction models developed without a correction for class imbalance consistently had equal or better calibration performance than prediction models developed with a correction for class imbalance. The miscalibration introduced by correcting for class imbalance was characterized by an over-estimation of risk and was not always able to be corrected with re-calibration. Correcting for class imbalance is not always necessary and may even be harmful for clinical prediction models which aim to produce reliable risk estimates on an individual basis.</p>
      </div>
     </dd>
     <dt>
      <a name="item18">[18]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19495" title="Abstract">arXiv:2404.19495</a> [<a href="/pdf/2404.19495" title="Download PDF">pdf</a>, <a href="/ps/2404.19495" title="Download PostScript">ps</a>, <a href="/format/2404.19495" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Percentage Coefficient (bp) -- Effect Size Analysis (Theory Paper 1)
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Zhao%2C+X">Xinshu Zhao</a> (1), <a href="/search/stat?searchtype=author&amp;query=Li%2C+D+M">Dianshi Moses Li</a> (2), <a href="/search/stat?searchtype=author&amp;query=Lai%2C+Z+Z">Ze Zack Lai</a> (1), <a href="/search/stat?searchtype=author&amp;query=Liu%2C+P+L">Piper Liping Liu</a> (3), <a href="/search/stat?searchtype=author&amp;query=Ao%2C+S+H">Song Harris Ao</a> (1), <a href="/search/stat?searchtype=author&amp;query=You%2C+F">Fei You</a> (1) ((1) Department of Communication, Faculty of Social Science, University of Macau, (2) Centre for Empirical Legal Studies, Faculty of Law, University of Macau, (3) School of Media and Communication, Shenzhen University)
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Econometrics (econ.EM); Methodology (stat.ME); Other Statistics (stat.OT)
       </div>
       <p class="mathjax">Percentage coefficient (bp) has emerged in recent publications as an additional and alternative estimator of effect size for regression analysis. This paper retraces the theory behind the estimator. It's posited that an estimator must first serve the fundamental function of enabling researchers and readers to comprehend an estimand, the target of estimation. It may then serve the instrumental function of enabling researchers and readers to compare two or more estimands. Defined as the regression coefficient when dependent variable (DV) and independent variable (IV) are both on conceptual 0-1 percentage scales, percentage coefficients (bp) feature 1) clearly comprehendible interpretation and 2) equitable scales for comparison. Thus, the coefficient (bp) serves both functions effectively and efficiently, thereby serving some needs not completely served by other indicators such as raw coefficient (bw) and standardized beta. Another fundamental premise of the functionalist theory is that "effect" is not a monolithic concept. Rather, it is a collection of compartments, each of which measures a component of the conglomerate that we call "effect." A regression coefficient (b), for example, measures one aspect of effect, which is unit effect, aka efficiency, as it indicates the unit change in DV associated with a one-unit increase in IV. Percentage coefficient (bp) indicates the change in DV in percentage points associated with a whole scale increase in IV. It is meant to be an all-encompassing indicator of the all-encompassing concept of effect, but rather an interpretable and comparable indicator of efficiency, one of the key components of effect.</p>
      </div>
     </dd>
     <dt>
      <a name="item19">[19]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19496" title="Abstract">arXiv:2404.19496</a> [<a href="/pdf/2404.19496" title="Download PDF">pdf</a>, <a href="/format/2404.19496" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Online and Offline Robust Multivariate Linear Regression
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/math?searchtype=author&amp;query=Godichon-Baggioni%2C+A">Antoine Godichon-Baggioni</a> (LPSM (UMR\_8001)), <a href="/search/math?searchtype=author&amp;query=Robin%2C+S+S">Stephane S. Robin</a> (LPSM (UMR\_8001)), <a href="/search/math?searchtype=author&amp;query=Sansonnet%2C+L">Laure Sansonnet</a> (MIA Paris-Saclay, LPSM (UMR\_8001))
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (stat.ML)
       </div>
       <p class="mathjax">We consider the robust estimation of the parameters of multivariate Gaussian linear regression models. To this aim we consider robust version of the usual (Mahalanobis) least-square criterion, with or without Ridge regularization. We introduce two methods each considered contrast: (i) online stochastic gradient descent algorithms and their averaged versions and (ii) offline fix-point algorithms. Under weak assumptions, we prove the asymptotic normality of the resulting estimates. Because the variance matrix of the noise is usually unknown, we propose to plug a robust estimate of it in the Mahalanobis-based stochastic gradient descent algorithms. We show, on synthetic data, the dramatic gain in terms of robustness of the proposed estimates as compared to the classical least-square ones. Well also show the computational efficiency of the online versions of the proposed algorithms. All the proposed algorithms are implemented in the R package RobRegression available on CRAN.</p>
      </div>
     </dd>
     <dt>
      <a name="item20">[20]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19528" title="Abstract">arXiv:2404.19528</a> [<a href="/pdf/2404.19528" title="Download PDF">pdf</a>, <a href="/format/2404.19528" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Tree P{ó}lya Splitting distributions for multivariate count data
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/math?searchtype=author&amp;query=Valiquette%2C+S">Samuel Valiquette</a> (UPR Forêts et Sociétés, Cirad-ES, UdeS, IMAG, LEMON), <a href="/search/math?searchtype=author&amp;query=Marchand%2C+%C3%89">Éric Marchand</a> (UdeS), <a href="/search/math?searchtype=author&amp;query=Peyhardi%2C+J">Jean Peyhardi</a> (IMAG), <a href="/search/math?searchtype=author&amp;query=Toulemonde%2C+G">Gwladys Toulemonde</a> (IMAG, LEMON), <a href="/search/math?searchtype=author&amp;query=Mortier%2C+F">Frédéric Mortier</a> (UPR Forêts et Sociétés, Cirad-ES, GEJP)
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>
       </div>
       <p class="mathjax">In this article, we develop a new class of multivariate distributions adapted for count data, called Tree P{\'o}lya Splitting. This class results from the combination of a univariate distribution and singular multivariate distributions along a fixed partition tree. As we will demonstrate, these distributions are flexible, allowing for the modeling of complex dependencies (positive, negative, or null) at the observation level. Specifically, we present the theoretical properties of Tree P{\'o}lya Splitting distributions by focusing primarily on marginal distributions, factorial moments, and dependency structures (covariance and correlations). The abundance of 17 species of Trichoptera recorded at 49 sites is used, on one hand, to illustrate the theoretical properties developed in this article on a concrete case, and on the other hand, to demonstrate the interest of this type of models, notably by comparing them to classical approaches in ecology or microbiome.</p>
      </div>
     </dd>
     <dt>
      <a name="item21">[21]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19557" title="Abstract">arXiv:2404.19557</a> [<a href="/pdf/2404.19557" title="Download PDF">pdf</a>, <a href="/format/2404.19557" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Neural Dynamic Data Valuation
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Liang%2C+Z">Zhangyong Liang</a>, <a href="/search/stat?searchtype=author&amp;query=Gao%2C+H">Huanhuan Gao</a>, <a href="/search/stat?searchtype=author&amp;query=Zhang%2C+J">Ji Zhang</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 43 pages, 19 figures
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)
       </div>
       <p class="mathjax">Data constitute the foundational component of the data economy and its marketplaces. Efficient and fair data valuation has emerged as a topic of significant interest.\ Many approaches based on marginal contribution have shown promising results in various downstream tasks. However, they are well known to be computationally expensive as they require training a large number of utility functions, which are used to evaluate the usefulness or value of a given dataset for a specific purpose. As a result, it has been recognized as infeasible to apply these methods to a data marketplace involving large-scale datasets. Consequently, a critical issue arises: how can the re-training of the utility function be avoided? To address this issue, we propose a novel data valuation method from the perspective of optimal control, named the neural dynamic data valuation (NDDV). Our method has solid theoretical interpretations to accurately identify the data valuation via the sensitivity of the data optimal control state. In addition, we implement a data re-weighting strategy to capture the unique features of data points, ensuring fairness through the interaction between data points and the mean-field states. Notably, our method requires only training once to estimate the value of all data points, significantly improving the computational efficiency. We conduct comprehensive experiments using different datasets and tasks. The results demonstrate that the proposed NDDV method outperforms the existing state-of-the-art data valuation methods in accurately identifying data points with either high or low values and is more computationally efficient.</p>
      </div>
     </dd>
     <dt>
      <a name="item22">[22]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19661" title="Abstract">arXiv:2404.19661</a> [<a href="/pdf/2404.19661" title="Download PDF">pdf</a>, <a href="/format/2404.19661" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> PCA for Point Processes
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Picard%2C+F">Franck Picard</a>, <a href="/search/stat?searchtype=author&amp;query=Rivoirard%2C+V">Vincent Rivoirard</a>, <a href="/search/stat?searchtype=author&amp;query=Roche%2C+A">Angelina Roche</a>, <a href="/search/stat?searchtype=author&amp;query=Panaretos%2C+V">Victor Panaretos</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (stat.ML)
       </div>
       <p class="mathjax">We introduce a novel statistical framework for the analysis of replicated point processes that allows for the study of point pattern variability at a population level. By treating point process realizations as random measures, we adopt a functional analysis perspective and propose a form of functional Principal Component Analysis (fPCA) for point processes. The originality of our method is to base our analysis on the cumulative mass functions of the random measures which gives us a direct and interpretable analysis. Key theoretical contributions include establishing a Karhunen-Lo\`{e}ve expansion for the random measures and a Mercer Theorem for covariance measures. We establish convergence in a strong sense, and introduce the concept of principal measures, which can be seen as latent processes governing the dynamics of the observed point patterns. We propose an easy-to-implement estimation strategy of eigenelements for which parametric rates are achieved. We fully characterize the solutions of our approach to Poisson and Hawkes processes and validate our methodology via simulations and diverse applications in seismology, single-cell biology and neurosiences, demonstrating its versatility and effectiveness. Our method is implemented in the pppca R-package.</p>
      </div>
     </dd>
     <dt>
      <a name="item23">[23]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19700" title="Abstract">arXiv:2404.19700</a> [<a href="/pdf/2404.19700" title="Download PDF">pdf</a>, <a href="/format/2404.19700" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Comparing Multivariate Distributions: A Novel Approach Using Optimal Transport-based Plots
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Singha%2C+S">Sibsankar Singha</a>, <a href="/search/stat?searchtype=author&amp;query=Kratz%2C+M">Marie Kratz</a>, <a href="/search/stat?searchtype=author&amp;query=Vadlamani%2C+S">Sreekar Vadlamani</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 45 pages, 21 figures
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Statistics Theory (math.ST)
       </div>
       <p class="mathjax">Quantile-Quantile (Q-Q) plots are widely used for assessing the distributional similarity between two datasets. Traditionally, Q-Q plots are constructed for univariate distributions, making them less effective in capturing complex dependencies present in multivariate data. In this paper, we propose a novel approach for constructing multivariate Q-Q plots, which extend the traditional Q-Q plot methodology to handle high-dimensional data. Our approach utilizes optimal transport (OT) and entropy-regularized optimal transport (EOT) to align the empirical quantiles of the two datasets. Additionally, we introduce another technique based on OT and EOT potentials which can effectively compare two multivariate datasets. Through extensive simulations and real data examples, we demonstrate the effectiveness of our proposed approach in capturing multivariate dependencies and identifying distributional differences such as tail behaviour. We also propose two test statistics based on the Q-Q and potential plots to compare two distributions rigorously.</p>
      </div>
     </dd>
    </dl>
    <h3>Cross-lists for Wed, 1 May 24</h3>
    <dl>
     <dt>
      <a name="item24">[24]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.17546" title="Abstract">arXiv:2404.17546</a> (cross-list from cs.LG) [<a href="/pdf/2404.17546" title="Download PDF">pdf</a>, <a href="/format/2404.17546" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+S">Stephen Zhao</a>, <a href="/search/cs?searchtype=author&amp;query=Brekelmans%2C+R">Rob Brekelmans</a>, <a href="/search/cs?searchtype=author&amp;query=Makhzani%2C+A">Alireza Makhzani</a>, <a href="/search/cs?searchtype=author&amp;query=Grosse%2C+R">Roger Grosse</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)
       </div>
       <p class="mathjax">Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.</p>
      </div>
     </dd>
     <dt>
      <a name="item25">[25]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.18979" title="Abstract">arXiv:2404.18979</a> (cross-list from econ.GN) [<a href="/pdf/2404.18979" title="Download PDF">pdf</a>, <a href="/format/2404.18979" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Analysis of Proximity Informed User Behavior in a Global Online Social Network
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/econ?searchtype=author&amp;query=Breitmar%2C+N">Nils Breitmar</a>, <a href="/search/econ?searchtype=author&amp;query=Harding%2C+M+C">Matthew C. Harding</a>, <a href="/search/econ?searchtype=author&amp;query=Zhang%2C+H">Hanqiao Zhang</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Applications (stat.AP)
       </div>
       <p class="mathjax">Despite the earlier claim of "Death of Distance", recent studies revealed that geographical proximity still greatly influences link formation in online social networks. However, it is unclear how physical distances are intertwined with users' online behaviors in a virtual world. We study the role of spatial dependence on a global online social network with a dyadic Logit model. Results show country-specific patterns for distance effect on probabilities to build connections. Effects are stronger when the possibility for two people to meet in person exists. Relative to weak ties, dependence on proximity is looser for strong social ties.</p>
      </div>
     </dd>
     <dt>
      <a name="item26">[26]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.18980" title="Abstract">arXiv:2404.18980</a> (cross-list from econ.GN) [<a href="/pdf/2404.18980" title="Download PDF">pdf</a>, <a href="/format/2404.18980" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> The Impact of COVID-19 on Co-authorship and Economics Scholars' Productivity
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/econ?searchtype=author&amp;query=Zhang%2C+H">Hanqiao Zhang</a>, <a href="/search/econ?searchtype=author&amp;query=Yang%2C+J+D+X">Joy D. Xiuyao Yang</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Physics and Society (physics.soc-ph); Applications (stat.AP)
       </div>
       <p class="mathjax">The COVID-19 pandemic has disrupted traditional academic collaboration patterns, prompting a unique opportunity to analyze the influence of peer effects and coauthorship dynamics on research output. Using a novel dataset, this paper endeavors to make a first cut at investigating the role of peer effects on the productivity of economics scholars, measured by the number of publications, in both pre-pandemic and pandemic times. Results show that peer effect is significant for the pre-pandemic time but not for the pandemic time. The findings contribute to our understanding of how research collaboration influences knowledge production and may help guide policies aimed at fostering collaboration and enhancing research productivity in the academic community.</p>
      </div>
     </dd>
     <dt>
      <a name="item27">[27]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.18992" title="Abstract">arXiv:2404.18992</a> (cross-list from hep-ph) [<a href="/pdf/2404.18992" title="Download PDF">pdf</a>, <a href="/format/2404.18992" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Unifying Simulation and Inference with Normalizing Flows
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/hep-ph?searchtype=author&amp;query=Du%2C+H">Haoxing Du</a>, <a href="/search/hep-ph?searchtype=author&amp;query=Krause%2C+C">Claudius Krause</a>, <a href="/search/hep-ph?searchtype=author&amp;query=Mikuni%2C+V">Vinicius Mikuni</a>, <a href="/search/hep-ph?searchtype=author&amp;query=Nachman%2C+B">Benjamin Nachman</a>, <a href="/search/hep-ph?searchtype=author&amp;query=Pang%2C+I">Ian Pang</a>, <a href="/search/hep-ph?searchtype=author&amp;query=Shih%2C+D">David Shih</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 11 pages, 7 figures
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; High Energy Physics - Experiment (hep-ex); Data Analysis, Statistics and Probability (physics.data-an); Instrumentation and Detectors (physics.ins-det); Machine Learning (stat.ML)
       </div>
       <p class="mathjax">There have been many applications of deep neural networks to detector calibrations and a growing number of studies that propose deep generative models as automated fast detector simulators. We show that these two tasks can be unified by using maximum likelihood estimation (MLE) from conditional generative models for energy regression. Unlike direct regression techniques, the MLE approach is prior-independent and non-Gaussian resolutions can be determined from the shape of the likelihood near the maximum. Using an ATLAS-like calorimeter simulation, we demonstrate this concept in the context of calorimeter energy calibration.</p>
      </div>
     </dd>
     <dt>
      <a name="item28">[28]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19112" title="Abstract">arXiv:2404.19112</a> (cross-list from cs.LG) [<a href="/pdf/2404.19112" title="Download PDF">pdf</a>, <a href="/format/2404.19112" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Hidden Synergy: $L_1$ Weight Normalization and 1-Path-Norm Regularization
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Biswas%2C+A">Aditya Biswas</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 8 pages body, 2 tables, 1 figure, 3 appendices
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)
       </div>
       <p class="mathjax">We present PSiLON Net, an MLP architecture that uses $L_1$ weight normalization for each weight vector and shares the length parameter across the layer. The 1-path-norm provides a bound for the Lipschitz constant of a neural network and reflects on its generalizability, and we show how PSiLON Net's design drastically simplifies the 1-path-norm, while providing an inductive bias towards efficient learning and near-sparse parameters. We propose a pruning method to achieve exact sparsity in the final stages of training, if desired. To exploit the inductive bias of residual networks, we present a simplified residual block, leveraging concatenated ReLU activations. For networks constructed with such blocks, we prove that considering only a subset of possible paths in the 1-path-norm is sufficient to bound the Lipschitz constant. Using the 1-path-norm and this improved bound as regularizers, we conduct experiments in the small data regime using overparameterized PSiLON Nets and PSiLON ResNets, demonstrating reliable optimization and strong performance.</p>
      </div>
     </dd>
     <dt>
      <a name="item29">[29]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19242" title="Abstract">arXiv:2404.19242</a> (cross-list from cs.CV) [<a href="/pdf/2404.19242" title="Download PDF">pdf</a>, <a href="/format/2404.19242" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> A Minimal Set of Parameters Based Depth-Dependent Distortion Model and Its Calibration Method for Stereo Vision Systems
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Ma%2C+X">Xin Ma</a>, <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+P">Puchen Zhu</a>, <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xiao Li</a>, <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+X">Xiaoyin Zheng</a>, <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+J">Jianshu Zhou</a>, <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xuchen Wang</a>, <a href="/search/cs?searchtype=author&amp;query=Au%2C+K+W+S">Kwok Wai Samuel Au</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> This paper has been accepted for publication in IEEE Transactions on Instrumentation and Measurement
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV); Methodology (stat.ME)
       </div>
       <p class="mathjax">Depth position highly affects lens distortion, especially in close-range photography, which limits the measurement accuracy of existing stereo vision systems. Moreover, traditional depth-dependent distortion models and their calibration methods have remained complicated. In this work, we propose a minimal set of parameters based depth-dependent distortion model (MDM), which considers the radial and decentering distortions of the lens to improve the accuracy of stereo vision systems and simplify their calibration process. In addition, we present an easy and flexible calibration method for the MDM of stereo vision systems with a commonly used planar pattern, which requires cameras to observe the planar pattern in different orientations. The proposed technique is easy to use and flexible compared with classical calibration techniques for depth-dependent distortion models in which the lens must be perpendicular to the planar pattern. The experimental validation of the MDM and its calibration method showed that the MDM improved the calibration accuracy by 56.55% and 74.15% compared with the Li's distortion model and traditional Brown's distortion model. Besides, an iteration-based reconstruction method is proposed to iteratively estimate the depth information in the MDM during three-dimensional reconstruction. The results showed that the accuracy of the iteration-based reconstruction method was improved by 9.08% compared with that of the non-iteration reconstruction method.</p>
      </div>
     </dd>
     <dt>
      <a name="item30">[30]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19261" title="Abstract">arXiv:2404.19261</a> (cross-list from cs.LG) [<a href="/pdf/2404.19261" title="Download PDF">pdf</a>, <a href="/format/2404.19261" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> High dimensional analysis reveals conservative sharpening and a stochastic edge of stability
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Agarwala%2C+A">Atish Agarwala</a>, <a href="/search/cs?searchtype=author&amp;query=Pennington%2C+J">Jeffrey Pennington</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Statistics Theory (math.ST); Data Analysis, Statistics and Probability (physics.data-an)
       </div>
       <p class="mathjax">Recent empirical and theoretical work has shown that the dynamics of the large eigenvalues of the training loss Hessian have some remarkably robust features across models and datasets in the full batch regime. There is often an early period of progressive sharpening where the large eigenvalues increase, followed by stabilization at a predictable value known as the edge of stability. Previous work showed that in the stochastic setting, the eigenvalues increase more slowly - a phenomenon we call conservative sharpening. We provide a theoretical analysis of a simple high-dimensional model which shows the origin of this slowdown. We also show that there is an alternative stochastic edge of stability which arises at small batch size that is sensitive to the trace of the Neural Tangent Kernel rather than the large Hessian eigenvalues. We conduct an experimental study which highlights the qualitative differences from the full batch phenomenology, and suggests that controlling the stochastic edge of stability can help optimization.</p>
      </div>
     </dd>
     <dt>
      <a name="item31">[31]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19274" title="Abstract">arXiv:2404.19274</a> (cross-list from cond-mat.dis-nn) [<a href="/pdf/2404.19274" title="Download PDF">pdf</a>, <a href="/ps/2404.19274" title="Download PostScript">ps</a>, <a href="/format/2404.19274" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Statistical Mechanics Calculations Using Variational Autoregressive Networks and Quantum Annealing
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cond-mat?searchtype=author&amp;query=Tamura%2C+Y">Yuta Tamura</a>, <a href="/search/cond-mat?searchtype=author&amp;query=Ohzeki%2C+M">Masayuki Ohzeki</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 5pages
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Machine Learning (stat.ML)
       </div>
       <p class="mathjax">In statistical mechanics, computing the partition function is generally difficult. An approximation method using a variational autoregressive network (VAN) has been proposed recently. This approach offers the advantage of directly calculating the generation probabilities while obtaining a significantly large number of samples. The present study introduces a novel approximation method that employs samples derived from quantum annealing machines in conjunction with VAN, which are empirically assumed to adhere to the Gibbs-Boltzmann distribution. When applied to the finite-size Sherrington-Kirkpatrick model, the proposed method demonstrates enhanced accuracy compared to the traditional VAN approach and other approximate methods, such as the widely utilized naive mean field.</p>
      </div>
     </dd>
     <dt>
      <a name="item32">[32]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19288" title="Abstract">arXiv:2404.19288</a> (cross-list from cs.LG) [<a href="/pdf/2404.19288" title="Download PDF">pdf</a>, <a href="/format/2404.19288" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Training-free Graph Neural Networks and the Power of Labels as Features
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Sato%2C+R">Ryoma Sato</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
       </div>
       <p class="mathjax">We propose training-free graph neural networks (TFGNNs), which can be used without training and can also be improved with optional training, for transductive node classification. We first advocate labels as features (LaF), which is an admissible but not explored technique. We show that LaF provably enhances the expressive power of graph neural networks. We design TFGNNs based on this analysis. In the experiments, we confirm that TFGNNs outperform existing GNNs in the training-free setting and converge with much fewer training iterations than traditional GNNs.</p>
      </div>
     </dd>
     <dt>
      <a name="item33">[33]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19292" title="Abstract">arXiv:2404.19292</a> (cross-list from cs.IT) [<a href="/pdf/2404.19292" title="Download PDF">pdf</a>, <a href="/format/2404.19292" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Provably Efficient Information-Directed Sampling Algorithms for Multi-Agent Reinforcement Learning
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Q">Qiaosheng Zhang</a>, <a href="/search/cs?searchtype=author&amp;query=Bai%2C+C">Chenjia Bai</a>, <a href="/search/cs?searchtype=author&amp;query=Hu%2C+S">Shuyue Hu</a>, <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhen Wang</a>, <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xuelong Li</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA); Machine Learning (stat.ML)
       </div>
       <p class="mathjax">This work designs and analyzes a novel set of algorithms for multi-agent reinforcement learning (MARL) based on the principle of information-directed sampling (IDS). These algorithms draw inspiration from foundational concepts in information theory, and are proven to be sample efficient in MARL settings such as two-player zero-sum Markov games (MGs) and multi-player general-sum MGs. For episodic two-player zero-sum MGs, we present three sample-efficient algorithms for learning Nash equilibrium. The basic algorithm, referred to as MAIDS, employs an asymmetric learning structure where the max-player first solves a minimax optimization problem based on the joint information ratio of the joint policy, and the min-player then minimizes the marginal information ratio with the max-player's policy fixed. Theoretical analyses show that it achieves a Bayesian regret of tilde{O}(sqrt{K}) for K episodes. To reduce the computational load of MAIDS, we develop an improved algorithm called Reg-MAIDS, which has the same Bayesian regret bound while enjoying less computational complexity. Moreover, by leveraging the flexibility of IDS principle in choosing the learning target, we propose two methods for constructing compressed environments based on rate-distortion theory, upon which we develop an algorithm Compressed-MAIDS wherein the learning target is a compressed environment. Finally, we extend Reg-MAIDS to multi-player general-sum MGs and prove that it can learn either the Nash equilibrium or coarse correlated equilibrium in a sample efficient manner.</p>
      </div>
     </dd>
     <dt>
      <a name="item34">[34]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19378" title="Abstract">arXiv:2404.19378</a> (cross-list from math.OC) [<a href="/pdf/2404.19378" title="Download PDF">pdf</a>, <a href="/ps/2404.19378" title="Download PostScript">ps</a>, <a href="/format/2404.19378" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Gaussian mixtures closest to a given measure via optimal transport
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/math?searchtype=author&amp;query=Lasserre%2C+J">Jean-Bernard Lasserre</a> (LAAS-POP, TSE-R)
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Statistics Theory (math.ST)
       </div>
       <p class="mathjax">Given a determinate (multivariate) probability measure $\mu$, we characterize Gaussian mixtures $\nu\_\phi$ which minimize the Wasserstein distance $W\_2(\mu,\nu\_\phi)$ to $\mu$ when the mixing probability measure $\phi$ on the parameters $(m,\Sigma)$ of the Gaussians is supported on a compact set $S$.(i) We first show that such mixtures are optimal solutions of a particular optimal transport (OT) problem where the marginal $\nu\_{\phi}$ of the OT problem is also unknown via the mixing measure variable $\phi$. Next (ii) by using a well-known specific property of Gaussian measures, this optimal transport is then viewed as a Generalized Moment Problem (GMP) and if the set $S$ of mixture parameters $(m,\Sigma)$ is a basic compact semi-algebraic set, we provide a "mesh-free" numerical scheme to approximate as closely as desired the optimal distance by solving a hierarchy of semidefinite relaxations of increasing size. In particular, we neither assume that the mixing measure is finitely supported nor that the variance is the same for all components. If the original measure $\mu$ is not a Gaussian mixture with parameters $(m,\Sigma)\in S$, then a strictly positive distance is detected at a finite step of the hierarchy. If the original measure $\mu$ is a Gaussian mixture with parameters $(m,\Sigma)\in S$, then all semidefinite relaxations of the hierarchy have same zero optimal value. Moreover if the mixing measure is atomic with finite support, its components can sometimes be extracted from an optimal solution at some semidefinite relaxation of the hierarchy when Curto &amp; Fialkow's flatness condition holds for some moment matrix.</p>
      </div>
     </dd>
     <dt>
      <a name="item35">[35]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19517" title="Abstract">arXiv:2404.19517</a> (cross-list from math.OC) [<a href="/pdf/2404.19517" title="Download PDF">pdf</a>, <a href="/ps/2404.19517" title="Download PostScript">ps</a>, <a href="/format/2404.19517" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Inexact subgradient methods for semialgebraic functions
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/math?searchtype=author&amp;query=Bolte%2C+J">Jérôme Bolte</a> (TSE-R), <a href="/search/math?searchtype=author&amp;query=Le%2C+T">Tam Le</a> (UGA, LJK), <a href="/search/math?searchtype=author&amp;query=Moulines%2C+%C3%89">Éric Moulines</a> (CMAP, MBZUAI), <a href="/search/math?searchtype=author&amp;query=Pauwels%2C+E">Edouard Pauwels</a> (TSE-R, IUF)
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (stat.ML)
       </div>
       <p class="mathjax">Motivated by the widespread use of approximate derivatives in machine learning and optimization, we study inexact subgradient methods with non-vanishing additive errors and step sizes. In the nonconvex semialgebraic setting, under boundedness assumptions, we prove that the method provides points that eventually fluctuate close to the critical set at a distance proportional to $\epsilon^\rho$ where $\epsilon$ is the error in subgradient evaluation and $\rho$ relates to the geometry of the problem. In the convex setting, we provide complexity results for the averaged values. We also obtain byproducts of independent interest, such as descent-like lemmas for nonsmooth nonconvex problems and some results on the limit of affine interpolants of differential inclusions.</p>
      </div>
     </dd>
     <dt>
      <a name="item36">[36]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19620" title="Abstract">arXiv:2404.19620</a> (cross-list from cs.LG) [<a href="/pdf/2404.19620" title="Download PDF">pdf</a>, <a href="/format/2404.19620" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Haoxuan Li</a>, <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+C">Chunyuan Zheng</a>, <a href="/search/cs?searchtype=author&amp;query=Ding%2C+S">Sihao Ding</a>, <a href="/search/cs?searchtype=author&amp;query=Wu%2C+P">Peng Wu</a>, <a href="/search/cs?searchtype=author&amp;query=Geng%2C+Z">Zhi Geng</a>, <a href="/search/cs?searchtype=author&amp;query=Feng%2C+F">Fuli Feng</a>, <a href="/search/cs?searchtype=author&amp;query=He%2C+X">Xiangnan He</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> ICLR 24
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR); Machine Learning (stat.ML)
       </div>
       <p class="mathjax">Selection bias in recommender system arises from the recommendation process of system filtering and the interactive process of user selection. Many previous studies have focused on addressing selection bias to achieve unbiased learning of the prediction model, but ignore the fact that potential outcomes for a given user-item pair may vary with the treatments assigned to other user-item pairs, named neighborhood effect. To fill the gap, this paper formally formulates the neighborhood effect as an interference problem from the perspective of causal inference and introduces a treatment representation to capture the neighborhood effect. On this basis, we propose a novel ideal loss that can be used to deal with selection bias in the presence of neighborhood effect. We further develop two new estimators for estimating the proposed ideal loss. We theoretically establish the connection between the proposed and previous debiasing methods ignoring the neighborhood effect, showing that the proposed methods can achieve unbiased learning when both selection bias and neighborhood effect are present, while the existing methods are biased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed methods.</p>
      </div>
     </dd>
     <dt>
      <a name="item37">[37]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19640" title="Abstract">arXiv:2404.19640</a> (cross-list from cs.LG) [<a href="/pdf/2404.19640" title="Download PDF">pdf</a>, <a href="/format/2404.19640" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Attacking Bayes: On the Adversarial Robustness of Bayesian Neural Networks
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Feng%2C+Y">Yunzhen Feng</a>, <a href="/search/cs?searchtype=author&amp;query=Rudner%2C+T+G+J">Tim G. J. Rudner</a>, <a href="/search/cs?searchtype=author&amp;query=Tsilivis%2C+N">Nikolaos Tsilivis</a>, <a href="/search/cs?searchtype=author&amp;query=Kempe%2C+J">Julia Kempe</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Methodology (stat.ME); Machine Learning (stat.ML)
       </div>
       <p class="mathjax">Adversarial examples have been shown to cause neural networks to fail on a wide range of vision and language tasks, but recent work has claimed that Bayesian neural networks (BNNs) are inherently robust to adversarial perturbations. In this work, we examine this claim. To study the adversarial robustness of BNNs, we investigate whether it is possible to successfully break state-of-the-art BNN inference methods and prediction pipelines using even relatively unsophisticated attacks for three tasks: (1) label prediction under the posterior predictive mean, (2) adversarial example detection with Bayesian predictive uncertainty, and (3) semantic shift detection. We find that BNNs trained with state-of-the-art approximate inference methods, and even BNNs trained with Hamiltonian Monte Carlo, are highly susceptible to adversarial attacks. We also identify various conceptual and experimental errors in previous works that claimed inherent adversarial robustness of BNNs and conclusively demonstrate that BNNs and uncertainty-aware Bayesian prediction pipelines are not inherently robust against adversarial attacks.</p>
      </div>
     </dd>
     <dt>
      <a name="item38">[38]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19649" title="Abstract">arXiv:2404.19649</a> (cross-list from cs.LG) [<a href="/pdf/2404.19649" title="Download PDF">pdf</a>, <a href="/format/2404.19649" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Landmark Alternating Diffusion
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Yeh%2C+S">Sing-Yuan Yeh</a>, <a href="/search/cs?searchtype=author&amp;query=Wu%2C+H">Hau-Tieng Wu</a>, <a href="/search/cs?searchtype=author&amp;query=Talmon%2C+R">Ronen Talmon</a>, <a href="/search/cs?searchtype=author&amp;query=Tsui%2C+M">Mao-Pei Tsui</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)
       </div>
       <p class="mathjax">Alternating Diffusion (AD) is a commonly applied diffusion-based sensor fusion algorithm. While it has been successfully applied to various problems, its computational burden remains a limitation. Inspired by the landmark diffusion idea considered in the Robust and Scalable Embedding via Landmark Diffusion (ROSELAND), we propose a variation of AD, called Landmark AD (LAD), which captures the essence of AD while offering superior computational efficiency. We provide a series of theoretical analyses of LAD under the manifold setup and apply it to the automatic sleep stage annotation problem with two electroencephalogram channels to demonstrate its application.</p>
      </div>
     </dd>
     <dt>
      <a name="item39">[39]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19707" title="Abstract">arXiv:2404.19707</a> (cross-list from econ.EM) [<a href="/pdf/2404.19707" title="Download PDF">pdf</a>, <a href="/format/2404.19707" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/econ?searchtype=author&amp;query=Virolainen%2C+S">Savi Virolainen</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics (econ.EM)</span>; Statistics Theory (math.ST); Methodology (stat.ME)
       </div>
       <p class="mathjax">Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production in times of low economic policy uncertainty but slightly increases it in times of high economic policy uncertainty. The introduced methods are implemented to the accompanying R package sstvars.</p>
      </div>
     </dd>
     <dt>
      <a name="item40">[40]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19719" title="Abstract">arXiv:2404.19719</a> (cross-list from cs.LG) [<a href="/pdf/2404.19719" title="Download PDF">pdf</a>, <a href="/format/2404.19719" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> The lazy (NTK) and rich ($?$P) regimes: a gentle tutorial
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Karkada%2C+D">Dhruva Karkada</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 22 pages, 7 figures
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)
       </div>
       <p class="mathjax">A central theme of the modern machine learning paradigm is that larger neural networks achieve better performance on a variety of metrics. Theoretical analyses of these overparameterized models have recently centered around studying very wide neural networks. In this tutorial, we provide a nonrigorous but illustrative derivation of the following fact: in order to train wide networks effectively, there is only one degree of freedom in choosing hyperparameters such as the learning rate and the size of the initial weights. This degree of freedom controls the richness of training behavior: at minimum, the wide network trains lazily like a kernel machine, and at maximum, it exhibits feature learning in the so-called $\mu$P regime. In this paper, we explain this richness scale, synthesize recent research results into a coherent whole, offer new perspectives and intuitions, and provide empirical evidence supporting our claims. In doing so, we hope to encourage further study of the richness scale, as it may be key to developing a scientific theory of feature learning in practical deep neural networks.</p>
      </div>
     </dd>
     <dt>
      <a name="item41">[41]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.19756" title="Abstract">arXiv:2404.19756</a> (cross-list from cs.LG) [<a href="/pdf/2404.19756" title="Download PDF">pdf</a>, <a href="/format/2404.19756" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> KAN: Kolmogorov-Arnold Networks
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Ziming Liu</a>, <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Y">Yixuan Wang</a>, <a href="/search/cs?searchtype=author&amp;query=Vaidya%2C+S">Sachin Vaidya</a>, <a href="/search/cs?searchtype=author&amp;query=Ruehle%2C+F">Fabian Ruehle</a>, <a href="/search/cs?searchtype=author&amp;query=Halverson%2C+J">James Halverson</a>, <a href="/search/cs?searchtype=author&amp;query=Solja%C4%8Di%C4%87%2C+M">Marin Solja?i?</a>, <a href="/search/cs?searchtype=author&amp;query=Hou%2C+T+Y">Thomas Y. Hou</a>, <a href="/search/cs?searchtype=author&amp;query=Tegmark%2C+M">Max Tegmark</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 48 pages, 20 figures. Codes are available at <a href="https://github.com/KindXiaoming/pykan">this https URL</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
       </div>
       <p class="mathjax">Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs.</p>
      </div>
     </dd>
    </dl>
    <h3>Replacements for Wed, 1 May 24</h3>
    <dl>
     <dt>
      <a name="item42">[42]</a>&nbsp; <span class="list-identifier"><a href="/abs/1511.05240" title="Abstract">arXiv:1511.05240</a> (replaced) [<a href="/pdf/1511.05240" title="Download PDF">pdf</a>, <a href="/ps/1511.05240" title="Download PostScript">ps</a>, <a href="/format/1511.05240" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> An extension of McDiarmid's inequality
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Combes%2C+R">Richard Combes</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 14 pages
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR); Statistics Theory (math.ST)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item43">[43]</a>&nbsp; <span class="list-identifier"><a href="/abs/2105.02797" title="Abstract">arXiv:2105.02797</a> (replaced) [<a href="/pdf/2105.02797" title="Download PDF">pdf</a>, <a href="/ps/2105.02797" title="Download PostScript">ps</a>, <a href="/format/2105.02797" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> The replica-symmetric free energy for Ising spin glasses with orthogonally invariant couplings
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/math?searchtype=author&amp;query=Fan%2C+Z">Zhou Fan</a>, <a href="/search/math?searchtype=author&amp;query=Wu%2C+Y">Yihong Wu</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Information Theory (cs.IT); Statistics Theory (math.ST)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item44">[44]</a>&nbsp; <span class="list-identifier"><a href="/abs/2112.10248" title="Abstract">arXiv:2112.10248</a> (replaced) [<a href="/pdf/2112.10248" title="Download PDF">pdf</a>, <a href="/format/2112.10248" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Efficient Modeling of Spatial Extremes over Large Geographical Domains
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Hazra%2C+A">Arnab Hazra</a>, <a href="/search/stat?searchtype=author&amp;query=Huser%2C+R">Raphaël Huser</a>, <a href="/search/stat?searchtype=author&amp;query=Bolin%2C+D">David Bolin</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>
       </div>
      </div>
     </dd>
     <dt>
      <a name="item45">[45]</a>&nbsp; <span class="list-identifier"><a href="/abs/2204.05933" title="Abstract">arXiv:2204.05933</a> (replaced) [<a href="/pdf/2204.05933" title="Download PDF">pdf</a>, <a href="/format/2204.05933" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Sparse Interaction Neighborhood Selection for Markov Random Fields via Reversible Jump and Pseudoposteriors
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Freguglia%2C+V">Victor Freguglia</a>, <a href="/search/stat?searchtype=author&amp;query=Garcia%2C+N+L">Nancy Lopes Garcia</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 27 pages
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Machine Learning (stat.ML)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item46">[46]</a>&nbsp; <span class="list-identifier"><a href="/abs/2206.08648" title="Abstract">arXiv:2206.08648</a> (replaced) [<a href="/pdf/2206.08648" title="Download PDF">pdf</a>, <a href="/format/2206.08648" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Orthonormal Expansions for Translation-Invariant Kernels
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/math?searchtype=author&amp;query=Tronarp%2C+F">Filip Tronarp</a>, <a href="/search/math?searchtype=author&amp;query=Karvonen%2C+T">Toni Karvonen</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 23 pages, 8 figures
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA); Machine Learning (stat.ML)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item47">[47]</a>&nbsp; <span class="list-identifier"><a href="/abs/2208.03215" title="Abstract">arXiv:2208.03215</a> (replaced) [<a href="/pdf/2208.03215" title="Download PDF">pdf</a>, <a href="/format/2208.03215" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Hierarchical Bayesian data selection
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Cotter%2C+S+L">Simon L. Cotter</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Methodology (stat.ME)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item48">[48]</a>&nbsp; <span class="list-identifier"><a href="/abs/2210.01757" title="Abstract">arXiv:2210.01757</a> (replaced) [<a href="/pdf/2210.01757" title="Download PDF">pdf</a>, <a href="/format/2210.01757" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Transportability of model-based estimands in evidence synthesis
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Remiro-Az%C3%B3car%2C+A">Antonio Remiro-Azócar</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 34 pages, 3 figures. Accepted for publication by Statistics in Medicine
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Applications (stat.AP)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item49">[49]</a>&nbsp; <span class="list-identifier"><a href="/abs/2210.05792" title="Abstract">arXiv:2210.05792</a> (replaced) [<a href="/pdf/2210.05792" title="Download PDF">pdf</a>, <a href="/format/2210.05792" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Flexible Modeling of Nonstationary Extremal Dependence using Spatially-Fused LASSO and Ridge Penalties
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Shao%2C+X">Xuanjie Shao</a>, <a href="/search/stat?searchtype=author&amp;query=Hazra%2C+A">Arnab Hazra</a>, <a href="/search/stat?searchtype=author&amp;query=Richards%2C+J">Jordan Richards</a>, <a href="/search/stat?searchtype=author&amp;query=Huser%2C+R">Raphaël Huser</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>
       </div>
      </div>
     </dd>
     <dt>
      <a name="item50">[50]</a>&nbsp; <span class="list-identifier"><a href="/abs/2210.13027" title="Abstract">arXiv:2210.13027</a> (replaced) [<a href="/pdf/2210.13027" title="Download PDF">pdf</a>, <a href="/format/2210.13027" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> E-Valuating Classifier Two-Sample Tests
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Pandeva%2C+T">Teodora Pandeva</a>, <a href="/search/stat?searchtype=author&amp;query=Bakker%2C+T">Tim Bakker</a>, <a href="/search/stat?searchtype=author&amp;query=Naesseth%2C+C+A">Christian A. Naesseth</a>, <a href="/search/stat?searchtype=author&amp;query=Forr%C3%A9%2C+P">Patrick Forré</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item51">[51]</a>&nbsp; <span class="list-identifier"><a href="/abs/2302.03435" title="Abstract">arXiv:2302.03435</a> (replaced) [<a href="/pdf/2302.03435" title="Download PDF">pdf</a>, <a href="/ps/2302.03435" title="Download PostScript">ps</a>, <a href="/format/2302.03435" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Logistic regression with missing responses and predictors: a review of existing approaches and a case study
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Martins%2C+S+R">Susana Rafaela Martins</a>, <a href="/search/stat?searchtype=author&amp;query=de+U%C3%B1a-%C3%81lvarez%2C+J">Jacobo de Uña-Álvarez</a>, <a href="/search/stat?searchtype=author&amp;query=del+Carmen+Iglesias-P%C3%A9rez%2C+M">María del Carmen Iglesias-Pérez</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 13 pages, 14 tables
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Applications (stat.AP)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item52">[52]</a>&nbsp; <span class="list-identifier"><a href="/abs/2303.02756" title="Abstract">arXiv:2303.02756</a> (replaced) [<a href="/pdf/2303.02756" title="Download PDF">pdf</a>, <a href="/format/2303.02756" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> A New Class of Realistic Spatio-Temporal Processes with Advection and Their Simulation
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Battagliola%2C+M+L">Maria Laura Battagliola</a>, <a href="/search/stat?searchtype=author&amp;query=Olhede%2C+S+C">Sofia Charlotta Olhede</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Methodology (stat.ME)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item53">[53]</a>&nbsp; <span class="list-identifier"><a href="/abs/2303.16660" title="Abstract">arXiv:2303.16660</a> (replaced) [<a href="/pdf/2303.16660" title="Download PDF">pdf</a>, <a href="/format/2303.16660" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Price Optimization Combining Conjoint Data and Purchase History: A Causal Modeling Approach
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Valkonen%2C+L">Lauri Valkonen</a>, <a href="/search/stat?searchtype=author&amp;query=Tikka%2C+S">Santtu Tikka</a>, <a href="/search/stat?searchtype=author&amp;query=Helske%2C+J">Jouni Helske</a>, <a href="/search/stat?searchtype=author&amp;query=Karvanen%2C+J">Juha Karvanen</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>
       </div>
      </div>
     </dd>
     <dt>
      <a name="item54">[54]</a>&nbsp; <span class="list-identifier"><a href="/abs/2303.17872" title="Abstract">arXiv:2303.17872</a> (replaced) [<a href="/pdf/2303.17872" title="Download PDF">pdf</a>, <a href="/ps/2303.17872" title="Download PostScript">ps</a>, <a href="/format/2303.17872" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Lancaster correlation -- a new dependence measure linked to maximum correlation
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Holzmann%2C+H">Hajo Holzmann</a>, <a href="/search/stat?searchtype=author&amp;query=Klar%2C+B">Bernhard Klar</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>
       </div>
      </div>
     </dd>
     <dt>
      <a name="item55">[55]</a>&nbsp; <span class="list-identifier"><a href="/abs/2305.18204" title="Abstract">arXiv:2305.18204</a> (replaced) [<a href="/pdf/2305.18204" title="Download PDF">pdf</a>, <a href="/format/2305.18204" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Kernel Density Matrices for Probabilistic Deep Learning
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Gonz%C3%A1lez%2C+F+A">Fabio A. González</a>, <a href="/search/cs?searchtype=author&amp;query=Ramos-Poll%C3%A1n%2C+R">Raúl Ramos-Pollán</a>, <a href="/search/cs?searchtype=author&amp;query=Gallego-Mejia%2C+J+A">Joseph A. Gallego-Mejia</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantum Physics (quant-ph); Machine Learning (stat.ML)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item56">[56]</a>&nbsp; <span class="list-identifier"><a href="/abs/2306.06327" title="Abstract">arXiv:2306.06327</a> (replaced) [<a href="/pdf/2306.06327" title="Download PDF">pdf</a>, <a href="/format/2306.06327" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Any-dimensional equivariant neural networks
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Levin%2C+E">Eitan Levin</a>, <a href="/search/cs?searchtype=author&amp;query=D%C3%ADaz%2C+M">Mateo Díaz</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 21 pages, 2 figures
       </div>
       <div class="list-journal-ref">
        <span class="descriptor">Journal-ref:</span> International Conference on Artificial Intelligence and Statistics. PMLR, 2024. Available from https://proceedings.mlr.press/v238/levin24a.html
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Representation Theory (math.RT); Machine Learning (stat.ML)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item57">[57]</a>&nbsp; <span class="list-identifier"><a href="/abs/2307.13301" title="Abstract">arXiv:2307.13301</a> (replaced) [<a href="/pdf/2307.13301" title="Download PDF">pdf</a>, <a href="/format/2307.13301" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Multiscale scanning with nuisance parameters
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=K%C3%B6nig%2C+C">Claudia König</a>, <a href="/search/stat?searchtype=author&amp;query=Munk%2C+A">Axel Munk</a>, <a href="/search/stat?searchtype=author&amp;query=Werner%2C+F">Frank Werner</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>
       </div>
      </div>
     </dd>
     <dt>
      <a name="item58">[58]</a>&nbsp; <span class="list-identifier"><a href="/abs/2308.00957" title="Abstract">arXiv:2308.00957</a> (replaced) [<a href="/pdf/2308.00957" title="Download PDF">pdf</a>, <a href="/format/2308.00957" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Causal Inference with Differentially Private (Clustered) Outcomes
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Javanmard%2C+A">Adel Javanmard</a>, <a href="/search/stat?searchtype=author&amp;query=Mirrokni%2C+V">Vahab Mirrokni</a>, <a href="/search/stat?searchtype=author&amp;query=Pouget-Abadie%2C+J">Jean Pouget-Abadie</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 41 pages, 10 figures
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Methodology (stat.ME)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item59">[59]</a>&nbsp; <span class="list-identifier"><a href="/abs/2309.08313" title="Abstract">arXiv:2309.08313</a> (replaced) [<a href="/pdf/2309.08313" title="Download PDF">pdf</a>, <a href="/format/2309.08313" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Conditional validity of heteroskedastic conformal regression
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Dewolf%2C+N">Nicolas Dewolf</a>, <a href="/search/stat?searchtype=author&amp;query=De+Baets%2C+B">Bernard De Baets</a>, <a href="/search/stat?searchtype=author&amp;query=Waegeman%2C+W">Willem Waegeman</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 36 pages
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item60">[60]</a>&nbsp; <span class="list-identifier"><a href="/abs/2312.16043" title="Abstract">arXiv:2312.16043</a> (replaced) [<a href="/pdf/2312.16043" title="Download PDF">pdf</a>, <a href="/format/2312.16043" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced linear classification
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Woo%2C+H">Hyenkyun Woo</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 26 pages, 9 figures, revised version
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item61">[61]</a>&nbsp; <span class="list-identifier"><a href="/abs/2401.11128" title="Abstract">arXiv:2401.11128</a> (replaced) [<a href="/pdf/2401.11128" title="Download PDF">pdf</a>, <a href="/format/2401.11128" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Regularized Estimation of Sparse Spectral Precision Matrices
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Deb%2C+N">Navonil Deb</a>, <a href="/search/stat?searchtype=author&amp;query=Kuceyeski%2C+A">Amy Kuceyeski</a>, <a href="/search/stat?searchtype=author&amp;query=Basu%2C+S">Sumanta Basu</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 55 pages, 8 figures
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Computation (stat.CO)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item62">[62]</a>&nbsp; <span class="list-identifier"><a href="/abs/2402.12785" title="Abstract">arXiv:2402.12785</a> (replaced) [<a href="/pdf/2402.12785" title="Download PDF">pdf</a>, <a href="/format/2402.12785" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Stochastic Graph Heat Modelling for Diffusion-based Connectivity Retrieval
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/eess?searchtype=author&amp;query=Goerttler%2C+S">Stephan Goerttler</a>, <a href="/search/eess?searchtype=author&amp;query=He%2C+F">Fei He</a>, <a href="/search/eess?searchtype=author&amp;query=Wu%2C+M">Min Wu</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 4 pages, 1 figure, conference paper
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Neurons and Cognition (q-bio.NC); Methodology (stat.ME)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item63">[63]</a>&nbsp; <span class="list-identifier"><a href="/abs/2403.08127" title="Abstract">arXiv:2403.08127</a> (replaced) [<a href="/pdf/2403.08127" title="Download PDF">pdf</a>, <a href="/ps/2403.08127" title="Download PostScript">ps</a>, <a href="/format/2403.08127" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Guidelines for the Creation of Analysis Ready Data
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Phillips%2C+H">Harriette Phillips</a>, <a href="/search/cs?searchtype=author&amp;query=Price%2C+A">Aiden Price</a>, <a href="/search/cs?searchtype=author&amp;query=Forbes%2C+O">Owen Forbes</a>, <a href="/search/cs?searchtype=author&amp;query=Boulange%2C+C">Claire Boulange</a>, <a href="/search/cs?searchtype=author&amp;query=Mengersen%2C+K">Kerrie Mengersen</a>, <a href="/search/cs?searchtype=author&amp;query=Reeves%2C+M">Marketa Reeves</a>, <a href="/search/cs?searchtype=author&amp;query=Glauert%2C+R">Rebecca Glauert</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 49 pages, 3 figures, 3 tables, and 5 appendices
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Data Analysis, Statistics and Probability (physics.data-an); Other Statistics (stat.OT)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item64">[64]</a>&nbsp; <span class="list-identifier"><a href="/abs/2403.14385" title="Abstract">arXiv:2403.14385</a> (replaced) [<a href="/pdf/2403.14385" title="Download PDF">pdf</a>, <a href="/format/2403.14385" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Estimating Causal Effects with Double Machine Learning -- A Method Evaluation
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Fuhr%2C+J">Jonathan Fuhr</a>, <a href="/search/stat?searchtype=author&amp;query=Berens%2C+P">Philipp Berens</a>, <a href="/search/stat?searchtype=author&amp;query=Papies%2C+D">Dominik Papies</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Methodology (stat.ME)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item65">[65]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.01413" title="Abstract">arXiv:2404.01413</a> (replaced) [<a href="/pdf/2404.01413" title="Download PDF">pdf</a>, <a href="/format/2404.01413" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Gerstgrasser%2C+M">Matthias Gerstgrasser</a>, <a href="/search/cs?searchtype=author&amp;query=Schaeffer%2C+R">Rylan Schaeffer</a>, <a href="/search/cs?searchtype=author&amp;query=Dey%2C+A">Apratim Dey</a>, <a href="/search/cs?searchtype=author&amp;query=Rafailov%2C+R">Rafael Rafailov</a>, <a href="/search/cs?searchtype=author&amp;query=Sleight%2C+H">Henry Sleight</a>, <a href="/search/cs?searchtype=author&amp;query=Hughes%2C+J">John Hughes</a>, <a href="/search/cs?searchtype=author&amp;query=Korbak%2C+T">Tomasz Korbak</a>, <a href="/search/cs?searchtype=author&amp;query=Agrawal%2C+R">Rajashree Agrawal</a>, <a href="/search/cs?searchtype=author&amp;query=Pai%2C+D">Dhruv Pai</a>, <a href="/search/cs?searchtype=author&amp;query=Gromov%2C+A">Andrey Gromov</a>, <a href="/search/cs?searchtype=author&amp;query=Roberts%2C+D+A">Daniel A. Roberts</a>, <a href="/search/cs?searchtype=author&amp;query=Yang%2C+D">Diyi Yang</a>, <a href="/search/cs?searchtype=author&amp;query=Donoho%2C+D+L">David L. Donoho</a>, <a href="/search/cs?searchtype=author&amp;query=Koyejo%2C+S">Sanmi Koyejo</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Emerging Technologies (cs.ET); Machine Learning (stat.ML)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item66">[66]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.08208" title="Abstract">arXiv:2404.08208</a> (replaced) [<a href="/pdf/2404.08208" title="Download PDF">pdf</a>, <a href="/format/2404.08208" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Shifting the Paradigm: Estimating Heterogeneous Treatment Effects in the Development of Walkable Cities Design
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Zhu%2C+J">Jie Zhu</a>, <a href="/search/stat?searchtype=author&amp;query=Liao%2C+B">Bojing Liao</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Methodology (stat.ME)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item67">[67]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.13177" title="Abstract">arXiv:2404.13177</a> (replaced) [<a href="/pdf/2404.13177" title="Download PDF">pdf</a>, <a href="/format/2404.13177" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> A Bayesian Hybrid Design with Borrowing from Historical Study
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/stat?searchtype=author&amp;query=Lu%2C+Z">Zhaohua Lu</a>, <a href="/search/stat?searchtype=author&amp;query=Toso%2C+J">John Toso</a>, <a href="/search/stat?searchtype=author&amp;query=Ayele%2C+G">Girma Ayele</a>, <a href="/search/stat?searchtype=author&amp;query=He%2C+P">Philip He</a>
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Applications (stat.AP)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item68">[68]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.15764" title="Abstract">arXiv:2404.15764</a> (replaced) [<a href="/pdf/2404.15764" title="Download PDF">pdf</a>, <a href="/format/2404.15764" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Assessment of the quality of a prediction
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/math?searchtype=author&amp;query=Sewell%2C+R">Roger Sewell</a>, <a href="/search/math?searchtype=author&amp;query=Crowe%2C+E">Elisabeth Crowe</a>, <a href="/search/math?searchtype=author&amp;query=Shariat%2C+S+F">Sharokh F. Shariat</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 16 pages, 3 figures; v2 has old reference 9 which is new reference 1 updated
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Methodology (stat.ME)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item69">[69]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.17489" title="Abstract">arXiv:2404.17489</a> (replaced) [<a href="/pdf/2404.17489" title="Download PDF">pdf</a>, <a href="/format/2404.17489" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Tabular Data Contrastive Learning via Class-Conditioned and Feature-Correlation Based Augmentation
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/cs?searchtype=author&amp;query=Cui%2C+W">Wei Cui</a>, <a href="/search/cs?searchtype=author&amp;query=Hosseinzadeh%2C+R">Rasa Hosseinzadeh</a>, <a href="/search/cs?searchtype=author&amp;query=Ma%2C+J">Junwei Ma</a>, <a href="/search/cs?searchtype=author&amp;query=Wu%2C+T">Tongzi Wu</a>, <a href="/search/cs?searchtype=author&amp;query=Sui%2C+Y">Yi Sui</a>, <a href="/search/cs?searchtype=author&amp;query=Golestan%2C+K">Keyvan Golestan</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 14 pages, 4 algorithms, 3 figures, 5 tables
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)
       </div>
      </div>
     </dd>
     <dt>
      <a name="item70">[70]</a>&nbsp; <span class="list-identifier"><a href="/abs/2404.17939" title="Abstract">arXiv:2404.17939</a> (replaced) [<a href="/pdf/2404.17939" title="Download PDF">pdf</a>, <a href="/format/2404.17939" title="Other formats">other</a>]</span>
     </dt>
     <dd>
      <div class="meta">
       <div class="list-title mathjax">
        <span class="descriptor">Title:</span> Control randomisation approach for policy gradient and application to reinforcement learning in optimal switching
       </div>
       <div class="list-authors">
        <span class="descriptor">Authors:</span> <a href="/search/math?searchtype=author&amp;query=Denkert%2C+R">Robert Denkert</a>, <a href="/search/math?searchtype=author&amp;query=Pham%2C+H">Huyên Pham</a>, <a href="/search/math?searchtype=author&amp;query=Warin%2C+X">Xavier Warin</a>
       </div>
       <div class="list-comments mathjax">
        <span class="descriptor">Comments:</span> 24 pages, 6 figures
       </div>
       <div class="list-subjects">
        <span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (stat.ML)
       </div>
      </div>
     </dd>
    </dl>
    <ul>
     <li><a href="/list/stat/new?skip=0&amp;show=2000">New submissions</a></li>
     <li><a href="#item24">Cross-lists</a></li>
     <li><a href="#item42">Replacements</a></li>
    </ul><small>[ total of 70 entries: <b>1-70</b> ]</small>
    <br><small>[ showing up to 2000 entries per page: <a href="/list/stat/new?skip=0&amp;show=1000">fewer</a> | <font color="#999999">more</font> ]</small>
    <br>
   </div>
   <br><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small>
   <script type="text/javascript" language="javascript">mathjaxToggle();</script>
   <hr>
   <p>Links to: <a href="/" accesskey="a">arXiv</a>, <a href="/form/stat">form interface</a>, <a href="/find/stat">find</a>, <a href="/archive/stat">stat</a>, <a href="/list/stat/recent">recent</a>, <a href="/list/stat/2404">2404</a>, <a href="/help/contact">contact</a>, <a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp; <small>(<a href="/help/accesskeys">Access key</a> information)</small></p>
   <hr>
  </div>
  <footer style="clear: both;">
   <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
    <!-- Macro-Column 1 -->
    <div class="column" style="padding: 0;">
     <div class="columns">
      <div class="column">
       <ul style="list-style: none; line-height: 2;">
        <li><a href="https://arxiv.org/about">About</a></li>
        <li><a href="https://arxiv.org/help">Help</a></li>
       </ul>
      </div>
      <div class="column">
       <ul style="list-style: none; line-height: 2;">
        <li>
         <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation">
          <title>
           contact arXiv
          </title><desc>
           Click here to contact arXiv
          </desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z" />
         </svg><a href="https://arxiv.org/help/contact"> Contact</a></li>
        <li>
         <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation">
          <title>
           subscribe to arXiv mailings
          </title><desc>
           Click here to subscribe
          </desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z" />
         </svg><a href="https://arxiv.org/help/subscribe"> Subscribe</a></li>
       </ul>
      </div>
     </div>
    </div><!-- End Macro-Column 1 --> <!-- Macro-Column 2 -->
    <div class="column" style="padding: 0;">
     <div class="columns">
      <div class="column">
       <ul style="list-style: none; line-height: 2;">
        <li><a href="https://arxiv.org/help/license">Copyright</a></li>
        <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
       </ul>
      </div>
      <div class="column sorry-app-links">
       <ul style="list-style: none; line-height: 2;">
        <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
        <li>
         <p class="help"><a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status 
           <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation">
            <path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z" />
           </svg></a><br>
           Get status notifications via <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank">
           <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation">
            <path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z" />
           </svg>email</a> or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank">
           <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation">
            <path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z" />
           </svg>slack</a></p></li>
       </ul>
      </div>
     </div>
    </div><!-- end MetaColumn 2 --> <!-- End Macro-Column 2 -->
   </div>
  </footer>
 </body>
</html>